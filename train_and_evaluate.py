# -*- coding: utf-8 -*-
"""
train_and_evaluate.py

Train the model on the training dataset, evaluate on the validation
dataset, and save the best model.

This script uses a WGAN-GP model (cWGAN) to train a generator and discriminator
on a dataset. The model is trained and evaluated on separate datasets, and the
best-performing models are saved for later inference or further training.

Author: Chengxi Yao
Email: stevenyao@g.skku.edu
"""

import argparse
import logging
import os
import shutil
from typing import Optional

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

import utils
import model.cWGAN as cgan
from model.dataloader import DepositDataset as DD


# Argument parsing for command-line arguments
parser = argparse.ArgumentParser()
parser.add_argument("--test_dir", default="tests/truecondW1",
                    help="Directory containing params.json")
parser.add_argument("--data_dir", default="data_raw/data/split_data",
                    help="Directory containing the dataset")
parser.add_argument("--gen_restore_file", default=None,
                    help="Optional generator weights to reload before training")
parser.add_argument("--disc_restore_file", default=None,
                    help="Optional discriminator weights to reload before training")


def calc_gradient_penalty(params, d_model, real_data, fake_data, conditions):
    """
    Calculate the gradient penalty for WGAN-GP.

    This function computes the gradient penalty term for the Wasserstein GAN with
    Gradient Penalty (WGAN-GP) to enforce the Lipschitz constraint in the discriminator.

    @param params: Model hyperparameters.
    @param d_model: Discriminator model.
    @param real_data: Real data samples.
    @param fake_data: Fake data generated by the generator.
    @param conditions: Conditioning variables for the conditional GAN.

    @return: The calculated gradient penalty.
    """
    alpha = torch.rand(real_data.size(0), 1, device=real_data.device)
    alpha = alpha.to(real_data.device)

    interpolates = alpha * real_data + (1 - alpha) * fake_data
    interpolates.requires_grad_(True)

    disc_interpolates = d_model(interpolates, conditions)

    gradients = torch.autograd.grad(
        outputs=disc_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones_like(disc_interpolates),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    gradients = gradients.view(gradients.size(0), -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * params.gp_lambda

    return gradient_penalty


def disc_train_step(params, d_model, g_model, d_optimizer, real_data, conditions):
    """
    One training step of the Discriminator model.

    This function trains the discriminator on both real data and fake data generated
    by the generator. The loss includes a gradient penalty term for WGAN-GP.

    @param params: Model hyperparameters.
    @param d_model: Discriminator model.
    @param g_model: Generator model.
    @param d_optimizer: Optimizer for the discriminator.
    @param real_data: Real data samples.
    @param conditions: Conditioning variables for the conditional GAN.

    @return: The discriminator loss value.
    """
    d_optimizer.zero_grad()

    z = torch.randn(conditions.size(0), params.z_dim, device=conditions.device)
    fake_data = g_model(z, conditions).detach()

    real_scores = d_model(real_data, conditions).mean()
    fake_scores = d_model(fake_data, conditions).mean()

    gradient_penalty = calc_gradient_penalty(params, d_model, real_data, fake_data, conditions)

    d_loss = fake_scores - real_scores + gradient_penalty
    d_loss.backward()
    d_optimizer.step()

    return d_loss.item()


def gen_train_step(params, d_model, g_model, g_optimizer, conditions):
    """
    One training step of the Generator model.

    This function trains the generator by minimizing the negative discriminator score
    for the generated (fake) data. This encourages the generator to produce more realistic
    data that fools the discriminator.

    @param params: Model hyperparameters.
    @param d_model: Discriminator model.
    @param g_model: Generator model.
    @param g_optimizer: Optimizer for the generator.
    @param conditions: Conditioning variables for the conditional GAN.

    @return: The generator loss value.
    """
    g_optimizer.zero_grad()

    z = torch.randn(conditions.size(0), params.z_dim, device=conditions.device)
    fake_data = g_model(z, conditions)

    fake_scores = d_model(fake_data, conditions).mean()

    g_loss = -fake_scores
    g_loss.backward()
    g_optimizer.step()

    return g_loss.item()


def print_model_info(g_model, d_model, conditions_batch, real_data_batch, z_dim):
    """
    Print model structure and input/output shapes for both the generator and discriminator.

    @param g_model: Generator model.
    @param d_model: Discriminator model.
    @param conditions_batch: A batch of condition inputs.
    @param real_data_batch: A batch of real data inputs.
    @param z_dim: Dimension of the latent space (noise input) for the generator.
    """
    print("Generator Model Structure:")
    print(g_model)

    print("\nDiscriminator Model Structure:")
    print(d_model)

    # Simulate a forward pass and print input/output shapes
    z = torch.randn(conditions_batch.size(0), z_dim, device=conditions_batch.device)
    with torch.no_grad():
        fake_data = g_model(z, conditions_batch)

    print(f"\nCondition batch shape: {conditions_batch.shape}")
    print(f"Real data batch shape: {real_data_batch.shape}")
    print(f"Fake data batch shape: {fake_data.shape}")


def train_epoch(g_model, d_model, g_optimizer, d_optimizer, dataloader, params):
    """
    Train the Generator and Discriminator for one epoch.

    This function loops over the dataset, training both the generator and discriminator
    models on each batch of data. It also computes key metrics such as MSE and Pearson
    Correlation Coefficient (PCC) for evaluation.

    @param g_model: Generator model.
    @param d_model: Discriminator model.
    @param g_optimizer: Optimizer for the generator.
    @param d_optimizer: Optimizer for the discriminator.
    @param dataloader: DataLoader for the training dataset.
    @param params: Model hyperparameters.

    @return: Training metrics, including generator loss, discriminator loss, MSE, and PCC.
    """
    g_model.train()
    d_model.train()

    g_loss_avg = utils.RunningAverage()
    d_loss_avg = utils.RunningAverage()
    mse_avg = utils.RunningAverage()
    pcc_avg = utils.RunningAverage()

    with tqdm(total=len(dataloader)) as t:
        for i, (conditions_batch, real_data_batch) in enumerate(dataloader):
            if params.cuda:
                conditions_batch = conditions_batch.cuda(non_blocking=True)
                real_data_batch = real_data_batch.cuda(non_blocking=True)

            # Train the Discriminator
            d_loss = disc_train_step(params, d_model, g_model, d_optimizer, real_data_batch, conditions_batch)
            d_loss_avg.update(d_loss)

            # Train the Generator every d_steps
            if i % params.d_steps == 0:
                g_loss = gen_train_step(params, d_model, g_model, g_optimizer, conditions_batch)
                g_loss_avg.update(g_loss)

            # Compute metrics
            with torch.no_grad():
                z = torch.randn(conditions_batch.size(0), params.z_dim, device=conditions_batch.device)
                fake_data = g_model(z, conditions_batch)
                mse = utils.compute_mse(real_data_batch, fake_data)
                pcc = utils.compute_pcc(real_data_batch, fake_data)
                mse_avg.update(mse)
                pcc_avg.update(pcc)

            # Update progress bar
            t.set_postfix(g_loss=f"{g_loss_avg():05.3f}", d_loss=f"{d_loss_avg():05.3f}",
                          mse=f"{mse_avg():05.3f}", pcc=f"{pcc_avg():.3f}")
            t.update()

        return {"g_loss": g_loss_avg(), "d_loss": d_loss_avg(), "mse": mse_avg(), "pcc": pcc_avg()}


def evaluate(g_model, val_dataloader, params):
    """
    Evaluate the Generator on the validation dataset.

    This function evaluates the generator by computing the generator loss, MSE,
    and PCC on the validation dataset.

    @param g_model: Generator model.
    @param val_dataloader: DataLoader for the validation dataset.
    @param params: Model hyperparameters.

    @return: Validation metrics, including generator loss, MSE, and PCC.
    """
    g_model.eval()

    val_g_loss_avg = utils.RunningAverage()
    mse_avg = utils.RunningAverage()
    pcc_avg = utils.RunningAverage()

    with torch.no_grad():
        for conditions_batch, real_data_batch in val_dataloader:
            if params.cuda:
                conditions_batch = conditions_batch.cuda(non_blocking=True)
                real_data_batch = real_data_batch.cuda(non_blocking=True)

            z = torch.randn(conditions_batch.size(0), params.z_dim, device=conditions_batch.device)
            fake_data = g_model(z, conditions_batch)

            # Compute losses and metrics
            g_loss = torch.nn.functional.mse_loss(fake_data, real_data_batch)
            mse = utils.compute_mse(real_data_batch, fake_data)
            pcc = utils.compute_pcc(real_data_batch, fake_data)

            val_g_loss_avg.update(g_loss.item())
            mse_avg.update(mse)
            pcc_avg.update(pcc)

    return {"g_loss": val_g_loss_avg(), "mse": mse_avg(), "pcc": pcc_avg()}


def train_and_evaluate(g_model, d_model, train_dataloader, val_dataloader,
                       g_optimizer, d_optimizer, params, test_dir):
    """
    Train and evaluate the model, and save the best model.

    This function trains the generator and discriminator models for multiple epochs,
    evaluates their performance on the validation set, and saves the best-performing
    models to disk.

    @param g_model: Generator model.
    @param d_model: Discriminator model.
    @param train_dataloader: DataLoader for the training dataset.
    @param val_dataloader: DataLoader for the validation dataset.
    @param g_optimizer: Optimizer for the generator.
    @param d_optimizer: Optimizer for the discriminator.
    @param params: Model hyperparameters.
    @param test_dir: Directory to save model checkpoints and metrics.
    """
    best_val_loss = float('inf')

    # Lists to store metrics
    g_losses, d_losses = [], []
    val_g_losses = []
    train_mses, val_mses = [], []
    train_pccs, val_pccs = [], []

    # Define checkpoint directory
    checkpoint_dir = os.path.join(test_dir, 'checkpoints')
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    for epoch in range(params.num_epochs):
        logging.info(f"Epoch {epoch + 1}/{params.num_epochs}")

        # Train the model
        train_metrics = train_epoch(g_model, d_model, g_optimizer, d_optimizer, train_dataloader, params)

        # Evaluate on validation set
        val_metrics = evaluate(g_model, val_dataloader, params)

        logging.info(f"Train G Loss: {train_metrics['g_loss']:.4f}, D Loss: {train_metrics['d_loss']:.4f}, "
                     f"MSE: {train_metrics['mse']:.4f}, PCC: {train_metrics['pcc']:.4f}")
        logging.info(f"Validation G Loss: {val_metrics['g_loss']:.4f}, "
                     f"MSE: {val_metrics['mse']:.4f}, PCC: {val_metrics['pcc']:.4f}")

        # Append metrics to lists
        g_losses.append(train_metrics['g_loss'])
        d_losses.append(train_metrics['d_loss'])
        val_g_losses.append(val_metrics['g_loss'])
        train_mses.append(train_metrics['mse'])
        val_mses.append(val_metrics['mse'])
        train_pccs.append(train_metrics['pcc'])
        val_pccs.append(val_metrics['pcc'])

        # Save the best model
        is_best = val_metrics["g_loss"] < best_val_loss
        if is_best:
            best_val_loss = val_metrics["g_loss"]
            utils.save_checkpoint({'epoch': epoch + 1, 'state_dict': g_model.state_dict(),
                                   'optim_dict': g_optimizer.state_dict()}, is_best, checkpoint_dir, model='gen')

            utils.save_checkpoint({'epoch': epoch + 1, 'state_dict': d_model.state_dict(),
                                   'optim_dict': d_optimizer.state_dict()}, is_best, checkpoint_dir, model='disc')

    # Plotting metrics
    epochs = range(1, params.num_epochs + 1)
    # Plot loss curves
    utils.plot_loss_curve(epochs, g_losses, d_losses, val_g_losses, test_dir)
    # Plot MSE curves
    utils.plot_metrics(epochs, [train_mses, val_mses], ['Train MSE', 'Validation MSE'],
                       'MSE', 'Mean Squared Error', 'mse_curve.png', test_dir)
    # Plot PCC curves
    utils.plot_metrics(epochs, [train_pccs, val_pccs], ['Train PCC', 'Validation PCC'],
                       'PCC', 'Pearson Correlation Coefficient', 'pcc_curve.png', test_dir)


if __name__ == "__main__":
    # Parse command-line arguments
    args = parser.parse_args()
    json_path = os.path.join(args.test_dir, "params.json")
    assert os.path.isfile(json_path), f"No json configuration file found at {json_path}"
    params = utils.Params(json_path)

    # Check if CUDA is available
    params.cuda = torch.cuda.is_available()
    torch.manual_seed(340)
    if params.cuda:
        torch.cuda.manual_seed(340)

    # Set up logging to both file and console
    utils.set_logger(os.path.join(args.test_dir, "train.log"))
    logging.info("Loading the datasets...")

    # Create datasets
    train_dataset = DD(os.path.join(args.data_dir, 'train.json'))
    val_dataset = DD(os.path.join(args.data_dir, 'valid.json'))

    # Create data loaders
    train_dl = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True,
                          num_workers=params.num_workers, pin_memory=params.cuda)
    val_dl = DataLoader(val_dataset, batch_size=params.batch_size, shuffle=False,
                        num_workers=params.num_workers, pin_memory=params.cuda)

    logging.info("- Done.")

    # Initialize models
    g_model = cgan.Generator(params).cuda() if params.cuda else cgan.Generator(params)
    d_model = cgan.Discriminator(params).cuda() if params.cuda else cgan.Discriminator(params)

    # Initialize optimizers
    g_optimizer = optim.Adam(g_model.parameters(), lr=params.learning_rate, betas=(params.beta1, 0.999))
    d_optimizer = optim.Adam(d_model.parameters(), lr=params.learning_rate, betas=(params.beta1, 0.999))

    # Optionally, restore model weights
    checkpoint_dir = os.path.join(args.test_dir, 'checkpoints')
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    if args.gen_restore_file is not None:
        gen_restore_path = os.path.join(checkpoint_dir, args.gen_restore_file)
        if os.path.isfile(gen_restore_path):
            utils.load_checkpoint(gen_restore_path, g_model, g_optimizer)
            logging.info(f"Restored Generator from {gen_restore_path}")
        else:
            # If the generator weights file is not found, log the message and proceed to train from scratch
            logging.info(f"Generator weights not found at {gen_restore_path}. Training from scratch.")

    if args.disc_restore_file is not None:
        disc_restore_path = os.path.join(checkpoint_dir, args.disc_restore_file)
        if os.path.isfile(disc_restore_path):
            utils.load_checkpoint(disc_restore_path, d_model, d_optimizer)
            logging.info(f"Restored Discriminator from {disc_restore_path}")
        else:
            # If the discriminator weights file is not found, log the message and proceed to train from scratch
            logging.info(f"Discriminator weights not found at {disc_restore_path}. Training from scratch.")

    # Print model information once before training
    # Get a batch of data
    example_conditions_batch, example_real_data_batch = next(iter(train_dl))
    if params.cuda:
        example_conditions_batch = example_conditions_batch.cuda(non_blocking=True)
        example_real_data_batch = example_real_data_batch.cuda(non_blocking=True)

    # Print model information
    print_model_info(g_model, d_model, example_conditions_batch, example_real_data_batch, params.z_dim)

    logging.info(f"Starting training for {params.num_epochs} epoch(s)")
    train_and_evaluate(g_model, d_model, train_dl, val_dl, g_optimizer, d_optimizer, params, args.test_dir)
